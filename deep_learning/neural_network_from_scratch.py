# -*- coding: utf-8 -*-
"""neural_network_from_scratch.ipynb

Automatically generated by Colaboratory.

# Neural Network Training
## Backpropagation Implementation Using Stochastic Gradient Descent

Task: Implementation of feedforward Neural Network using NumPy.

Implement a “Class” to represent your Neural Network.
"""

import numpy as np
import random

class Neuron():
  '''
  A class which represents a neuron 
  '''
  def __init__(self, input_size, zero_bias):
    
    self.input_size = input_size
    self.weights = np.random.normal(size=input_size)
    if not zero_bias:
      self.biases = np.random.normal(size=input_size)
    else:
      self.biases = [0 for i in range(input_size)]
    
    # to record change for each neuron input / dendrite
    self.nablas_b = np.zeros(self.input_size)
    self.nablas_w = np.zeros(self.input_size)

    self.z = None # value of neuron before sigmoid
    self.value = None # value of neuron after sigmoid

  def sigmoid(self, x):
    return 1 / (1 + np.exp(-x))
  
  def sigmoid_derivative(self, x):
    return self.sigmoid(x) * (1 - self.sigmoid(x))

  def learn(self, learning_rate, batch_size):
    for i in range(self.input_size):
      self.weights[i] = self.weights[i] - (learning_rate / batch_size) * self.nablas_w[i]
      self.biases[i] = self.biases[i] - (learning_rate / batch_size) * self.nablas_b[i]
      

  def activate(self, inputs):
    if len(inputs) != self.input_size:
      raise Exception('This neuron expects %d inputs but %d inputs were provided' % (self.input_size, len(inputs)))
    self.z = sum([self.weights[i] * inputs[i] + self.biases[i] for i in range(self.input_size)])
    self.value = self.sigmoid(self.z)
    return self.value
 
  def __str__(self):
    text = ''
    for i in range(self.input_size):
      text += 'w: %f,\t b: %f\n' % (self.weights[i], self.biases[i])
    return text

class NN():
  
  def __init__(self, layers_info):
    '''
    A constructor to initialize the network size in the Python list. The list 
    should contain the number of neurons in the respective layers of the 
    network. For example, if the list was [2, 3, 1] then it would be a 
    three-layer network, with the first layer containing 2 neurons, the second 
    layer 3 neurons, and the third layer 1 neuron. The biases and weights for 
    the network are initialized randomly, using a Gaussian distribution with 
    mean 0, and variance 1. Note that the first layer is assumed to be an input
    layer, and by convention we won't set any biases for those neurons, since
    biases are only ever used in computing the outputs from later layers.
    '''
    self.layers = list()
    for layer_no, neurons in enumerate(layers_info):
      layer = list()
      for i in range(neurons):
        input_size = layers_info[layer_no - 1] if layer_no != 0 else 1          
        neuron = Neuron(input_size, zero_bias = True if layer_no == 0 else False)
        layer.append(neuron)
      self.layers.append(layer)

  def feed_forward(self, data):
    '''
    A FEEDFORWARD member function to return the output of the network.
    '''
    layer_output = list()
    for layer_no, layer in enumerate(self.layers):
      outputs = list()
      for neuron_no, neuron in enumerate(layer):
        if layer_no == 0:
          inputs = [data[neuron_no]]
        else:
          inputs = layer_output 
        output = neuron.activate(inputs)
        outputs.append(output)
      layer_output = outputs
    return layer_output[0]

  def sgd(self, training_data, epochs, mini_batch_size, learning_rate, test_data):
    '''
    The SGD member function to train the neural network using mini-batch 
    stochastic gradient descent. For this, you may assume that the training
    data is a list of tuples (x, y) representing the training inputs and the
    desired outputs.'''

    for i in range(epochs):
      mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, len(training_data), mini_batch_size)]
      random.shuffle(mini_batches)
      for mini_batch in mini_batches:
        self.update_parameters(mini_batch, learning_rate)
        if test_data:
          print("Epoch %s: %s / %s" % (i, self.evaluate(test_data), len(test_data)))
        else:
          print("Epoch %s complete" % i)


  def update_parameters(self, batch, learning_rate):
    '''
    The update member function to update the network's weights and biases by 
    applying gradientdescent using backpropagation to a single mini batch.
    '''
    for x,y in batch:
      self.backprop(x, y)
    
    # update weights / biases for each neuron. Make neurons learn the pattern :)
    for layer_no, layer in enumerate(self.layers):
      for neuron_no, neuron in enumerate(layer):
        neuron.learn(learning_rate, len(batch))


  def backprop(self, x, y):

    '''
    The BACKPROP member function to return a tuple (nabla_b, nabla_w) 
    representing the gradient for the cost function. nabla_b and nabla_w may be
    the layer-by-layer lists of numpy arrays, similar to self.biases and 
    self.weights defined in the constructor.
    '''

    for layer_no in range(len(self.layers) - 1, -1, -1):
      layer = self.layers[layer_no]
      for neuron_no in range(len(self.layer) - 1, -1, -1):
        neuron = self.layer[neuron_no]
        
        if layer_no == len(self.layers) - 1: # pass derivate as it is from last layer
          delta = self.cost_derivative(neuron.value, y) * neuron.sigmoid_derivative()
          
        else:
          delta = np.dot(neuron.weights, delta) * neuron.sigmoid_derivative()

        # Record partial derivative of weight and bias for each individual neuron in every layer
        neuron.nablas_b = [delta for i in range(neuron.input_size)]
        neuron.nablas_w = [np.dot(neuron.value, delta) for neuron in self.layers[layer_no-1]]
  

  def evaluate(self, test_data):        
    test_results = [(np.argmax(self.feed_forward(x)), y)
                        for (x, y) in test_data]
    return sum(int(x == y) for (x, y) in test_results)

  def cost_derivative(self, predicted_output, real_output):
    return predicted_output - real_output
      
  def __str__(self):
    info = ''
    for layer_no, layer in enumerate(self.layers):
      info += 'Layer#:%2d, Neurons: %02d\n' % (layer_no + 1, len(layer))
      for neuron_no, neuron in enumerate(layer):
        info += 'Neuron#' + str(neuron_no+ 1) + '\n' + str(neuron) + "\n"
      info += "\n"
    return info

nn = NN([2, 3, 1])
print(nn)
print(nn.feed_forward([1 , 1]))
#nn.sgd(traning_date, epoch, mini_batch_size, learning_rate)
